#include '../ast/node.jch'
#include './engine.jch'

ExecSession! ama::CreateSession(Node* nd_entry,Node*+[...] interests){
	//TODO: expand ifs containing interest right away? just rely on on-demand expansion for now
	//the sole query to support is "lookup"
	ret=new ExecSession!
	for nd_interest in interests
		ret.AddInterest(nd_interest)
	ret.entry=ret.CreateNodeGraph(nd_entry)
	return <<ret
}

void ExecSession::AddInterest(Node* nd_interest){
	kind=EINTEREST_VAR
	name=nd_interest.GetName();
	if nd_interest.node_class==N_FUNCTION:
		kind=EINTEREST_FUNCTION
	else
		assert(nd_interest.node_class==N_REF)
		if nd_interest.Owner().node_class==N_CLASS:
			kind=EINTEREST_FIELD
	nd_lexical_scope=nd_interest.Owning(N_SCOPE)
	if !nd_lexical_scope:
		nd_lexical_scope=nd_interest.Root()
	this.interests.push(new ExecInterest{
		kind:kind,
		nd_interest:nd_interest,
		nd_lexical_scope:nd_lexical_scope,
		same_name_next:this.name_to_interest[name]-1L
	})
	this.name_to_interest[name]=this.interests.length
	//TODO: recheck existing nodes: can we update the TMPF_CFG_MATTERS_FOR_INTEREST flag incrementally?
}

private void CollectLabels(Map<char[|],Node*+>&+! labels,Node* nd_entry){
	for(ndi=nd_entry;ndi;ndi=ndi.PreorderNext(nd_entry))
		if ndi!=nd_entry&&ndi.GetCFGRole()==CFG_DECL:
			ndi=ndi.PreorderLastInside()
			continue
		if ndi.node_class==N_LABELED&&ndi.c.node_class==N_REF&&!(ndi.p.node_class==N_RAW&&(ndi.p.flags&0xffff)):
			labels[ndi.data]=ndi.c.s
}


private void CollectComputedLabels(Node*+[+]&+! addressed_labels,Map<char[|],Node*+>&+! labels,Node* nd_entry){
	for(ndi=nd_entry;ndi;ndi=ndi.PreorderNext(nd_entry))
		if ndi!=nd_entry&&ndi.GetCFGRole()==CFG_DECL:
			ndi=ndi.PreorderLastInside()
			continue
		if ndi.node_class==N_UNARY&&!(ndi.flags&UNARY_POSTFIX)&&ndi.data=='&&'&&ndi.c.node_class==N_REF:
			nd_target=labels[ndi.c.data]
			if nd_target:
				addressed_labels.push_back(nd_target)
	addressed_labels.sortby(nd=>intptr_t(nd))
	addressed_labels.unique()
}

private const intptr_t EXEC_BLOCK_SIZE=65536L-64L
ExecNode*+ ExecSession::CreateExecNode(uint8_t kind,Node*+ nd_exec){
	ExecNode*+ ret=poolAlloc(&this.pool, sizeof(ExecNode), EXEC_BLOCK_SIZE);
	ret.kind=kind
	ret.nd_exec=nd_exec
	return ret
}

void ExecSession::AddLinkTo(ExecNodeLinks*+ plinks,ExecNode*+ ed_target){
	if !plinks.fast[0]:
		plinks.fast[0]=ed_target
		return
	if !plinks.fast[1]:
		plinks.fast[1]=plinks.fast[0]
		plinks.fast[0]=ed_target
		return
	ExecNodeExtraLink*+ new_link=poolAlloc(&this.pool, sizeof(ExecNodeExtraLink), EXEC_BLOCK_SIZE);
	new_link.target=plinks.fast[1]
	new_link.next=plinks.more
	plinks.more=new_link
	plinks.fast[1]=plinks.fast[0]
	plinks.fast[0]=ed_target
}

void ExecSession::AddEdge(ExecNode*+ ed0,ExecNode*+ ed1){
	//COULDDO: check for existing edge
	//the caching mechanism shouldn't generate duplicate edges...
	this.AddLinkTo(&ed0.next,ed1)
	this.AddLinkTo(&ed1.prev,ed0)
}

private const uint16_t TMPF_CONTAINS_INTEREST=1u;
private const uint16_t TMPF_CFG_MATTERS_FOR_INTEREST=2u;
private const int32_t JSCOPE_PARENT_DECL=1;
private const int32_t JSCOPE_PARENT_SWITCH=2;
private const int32_t JSCOPE_PARENT_LOOP=4;
///dfsCreateNodeGraph relies on CreateNodeGraph for temp flag setting
ExecRange! ExecSession::dfsCreateNodeGraph(Node*+ nd,Node*+ nd_entry,int cached){
	if cached:
		ExecRange! rg_canon=<<copy(this.canonicals[nd].rg_canon)
		if rg_canon.entry:return <<copy(rg_canon)
	uint16_t flags=0
	role=nd.GetCFGRole();
	//nd_entry could be anything: we may end up calling this expanding some other node
	switch role{
	default:
		assert(0);
		break
	case CFG_DECL:
		if nd==nd_entry:
			//get into nd_entry no matter what
			ed_enter=this.CreateExecNode(ENODE_NORMAL,nd)
			nd_scope=role.Find(N_SCOPE,NULL)
			if nd_scope:
				ExecRange! rg_scope=dfsCreateNodeGraph(nd_scope,nd_entry,cached)
				this.AddEdge(rg_scope.exit,ed_scope)
				rg_scope.exit=ed_scope
				if cached:
					this.canonicals[nd].rg_canon=<<copy(rg_scope)
				return <<copy(rg_scope)
		//otherwise, simply ignore: create a dumb node
		break 
	case CFG_BASIC:
		if nd.node_class==N_SCOPE:
			ExecNode*+ ed_last=NULL
			ExecNode*+ ed_first=NULL
			for(auto ndi=nd.c;ndi;ndi=ndi.s)
				ExecRange! rgi=this.dfsCreateNodeGraph(ndi,nd_entry,cached)
				if ed_last:
					this.AddEdge(ed_last,rgi.entry)
				else
					ed_first=rgi.entry
				ed_last=rgi.exit
			//create the end-of-scope node
			ExecNode*+ ed_after=NULL
			ExecNodeCanonicals*+ p_canon=NULL
			if cached:
				p_canon=&this.canonicals[nd]
				ed_after=p_canon.ed_after
			if !ed_after:
				ed_after=this.CreateExecNode(ENODE_AFTER,nd)
			if ed_last:
				this.AddEdge(ed_last,ed_after)
			else
				ed_first=ed_after
			if cached:
				p_canon.rg_normal=new ExecRange{entry:ed_first,exit:ed_after}
				p_canon.ed_after=ed_after
			return new ExecRange{entry:ed_first,exit:ed_after}
		else if nd.tmp_flags&TMPF_CFG_MATTERS_FOR_INTEREST:
			//a basic node could contain a branch inside: a=(foo&&bar?baz():0)
			//thus the effects of a node should exclude its contained branches
			Node*+[+] non_basic_children
			for(auto ndi=nd;ndi;ndi=ndi.PreorderNext(nd))
				role_i=ndi.GetCFGRole();
				if role_i!=CFG_BASIC:
					non_basic_children.push_back(ndi)
					ndi=ndi.PreorderLastInside()
					continue
			if non_basic_children.length>1:
				//use fork / join to represent non-deterministic order
				auto ed_fork=this.CreateExecNode(ENODE_FORK,NULL)
				auto ed_join=this.CreateExecNode(ENODE_JOIN,NULL)
				for ndi in non_basic_children
					ExecRange! rgi=this.dfsCreateNodeGraph(ndi,nd_entry,cached)
					this.AddEdge(ed_fork,rgi.entry)
					this.AddEdge(rgi.exit,ed_join)
				auto ed_self=this.CreateExecNode(ENODE_NORMAL,nd)
				this.AddEdge(ed_join,ed_self)
				return new ExecRange{entry:ed_fork,exit:ed_self}
			else if non_basic_children.length==1:
				//this one is easy: just put non_basic_children[0] before nd
				ExecRange! rgi=this.dfsCreateNodeGraph(non_basic_children[0],nd_entry,cached)
				auto ed_self=this.CreateExecNode(ENODE_NORMAL,nd)
				this.AddEdge(rgi.exit,ed_self)
				rgi.exit=ed_self
				return <<copy(rgi)
		//otherwise, create a dumb node
		break;
	case CFG_JUMP:
		//TODO: goto target problem: "canonical" ExecNode for each Node, and the after version
		break
	case CFG_BRANCH:
		if !(nd.tmp_flags&TMPF_CFG_MATTERS_FOR_INTEREST):
			flags=ENODEF_FOLDED_CFG
			break
		//TODO:
		break
	case CFG_LOOP:
		if !(nd.tmp_flags&TMPF_CFG_MATTERS_FOR_INTEREST):
			flags=ENODEF_FOLDED_CFG
			break
		//TODO:
		break
	}
	//fall back path: create a normal node
	ExecNode*+ ed=this.CreateExecNode(ENODE_NORMAL,nd)
	ed.flags=flags
	return new ExecRange{entry:ed,exit:ed}
}

ExecRange! ExecSession::CreateNodeGraph(Node* nd_entry,int cached){
	//TODO: separate nd_entry from nd_parent_of_goto
	labels_collected=0
	labels=new Map<char[|],Node*+>
	addressed_labels=new Node*+[+]
	jump_targets=new Map<Node*+,Node*+>
	for(ndi=nd_entry;ndi;ndi=ndi.PreorderNext(nd_entry))
		if ndi!=nd_entry&&ndi.GetCFGRole()==CFG_DECL:
			ndi=ndi.PreorderLastInside()
			continue
		//preorder means this initialization happens before we flag a parent as to-expand
		ndi.tmp_flags&=~TMPF_CONTAINS_INTEREST
		//if we see an interest, we need to expand parents
		if ndi.node_class==N_REF||ndi.node_class==N_DOT:
			!? //TODO: incrementalize this logic
			first=this.name_to_interest[ndi.data]-1L
			for(i=first;i>=0L;i=this.interests[i].same_name_next)
				//COULDDO: more accurate name resolution for dot / function
				//we can always rule out an effect later as impossible
				uint8_t sure=SURE_NEVER
				if this.interests[i].kind==N_REF&&ndi.node_class==N_REF&&this.interests[i].nd_lexical_scope.isAncestorOf(ndi):
					sure=SURE_ALWAYS
				else if this.interests[i].kind==N_DOT&&ndi.node_class==N_DOT:
					this.uncertainty|=UNCERTAIN_FIELD_RESOLUTION
					sure=SURE_MAYBE
				else if this.interests[i].kind==N_FUNCTION&&ndi.p&&ndi==ndi.p.c&&ndi.p.node_class==N_CALL:
					//COULDDO: improve this
					this.uncertainty|=UNCERTAIN_FUNCTION_RESOLUTION
					sure=SURE_MAYBE
				if sure!=SURE_NEVER:
					//we got an interest, mark parents as to-expand
					auto new_flags=TMPF_CONTAINS_INTEREST
					Node*+ ndj_last=NULL
					for(ndj=ndi;ndj;ndj=ndj.p)
						if ndj_last&&ndj.isChildCFGDependent(ndj_last)
							new_flags|=TMPF_CFG_MATTERS_FOR_INTEREST
						if (ndj.tmp_flags&new_flags)==new_flags:break
						ndj.tmp_flags|=new_flags
						if ndj==nd_entry:break
						ndj_last=ndj
		else if ndi.GetCFGRole()==CFG_JUMP:
			//resolve jump target
			//which may not always work, but we can mark unknown jumps as always interesting
			assert(ndi.node_class==N_KEYWORD_STATEMENT)
			Node*+ nd_target=NULL
			if ndi.data=='goto'||(ndi.data=='break'||ndi.data=='continue')&&ndi.c&&ndi.c.node_class==N_REF:
				//collect labels lazily if we see goto
				if !labels_collected:
					labels_collected=1
					CollectLabels(labels,nd_entry)
				if ndi.c&&ndi.c.node_class==N_REF:
					nd_target=labels[ndi.c.data]
					if ndi.data!='goto'&&nd_target:
						//were labeled break / continue to fail, we try the normal logic
						nd_target=nd_target.Find(N_SCOPE,NULL)
				else
					//computed goto
					this.uncertainty|=UNCERTAIN_JUMP_TARGET
					if labels_collected<2:
						labels_collected=2
						CollectComputedLabels(addressed_labels,labels,nd_entry)
					!? //TODO: mark all addressed_labels as target... implicitly, mark computed goto and target common ancestor
			if !nd_target&&ndi.data!='goto':
				//break / continue / return: there is no target node
				//we only need common ancestor and target execution node... point to the loop / function scope
				//all jumps can use the _AFTER positioning
				parent_mode=JSCOPE_PARENT_DECL
				if ndi.data=='return'||ndi.data=='throw':
					//do nothing
				else
					parent_mode|=JSCOPE_PARENT_LOOP
					if ndi.data=='break':
						parent_mode|=JSCOPE_PARENT_SWITCH
					else
						assert(ndi.data=='continue')
				for(ndj=ndi;ndj;ndj=ndj.p)
					if ndj.node_class!=N_SCOPE:continue
					//take the outer-most scope if we can't find a correct one
					nd_target=ndj
					nd_parent=ndj.p
					if nd_parent:break
					if ndj==nd_entry:break
					auto role=nd_parent.GetCFGRole()
					if (parent_mode&JSCOPE_PARENT_DECL)&&role==CFG_DECL:break
					if (parent_mode&JSCOPE_PARENT_LOOP)&&role==CFG_LOOP:break
					if (parent_mode&JSCOPE_PARENT_SWITCH)&&nd_parent.node_class==N_SCOPED_STATEMENT&&nd_parent.data=='switch':
						break
			if !nd_target:
				//assume nop-jump
				nd_target=ndi
			jump_targets[ndi]=nd_target
	//propagate TMPF_CFG_MATTERS_FOR_INTEREST to JUMPs crossing them: common ancestor test
	for(;;)
		jump_mattered=0
		for nd_target,nd_source in jump_targets
			auto nd_jump_range=nd_source.CommonAncestor(nd_target)
			if nd_entry.isAncestorOf(nd_jump_range)&&(nd_jump_range.tmp_flags&TMPF_CFG_MATTERS_FOR_INTEREST):
				//we got a cross-interest jump, mark both side's parents as to-expand
				auto new_flags=TMPF_CONTAINS_INTEREST|TMPF_CFG_MATTERS_FOR_INTEREST
				Node*+ ndj_last=NULL
				for(ndj=nd_source;ndj;ndj=ndj.p)
					if (ndj.tmp_flags&new_flags)==new_flags:break
					ndj.tmp_flags|=new_flags
					jump_mattered=1
					if ndj==nd_entry:break
					ndj_last=ndj
				for(ndj=nd_target;ndj;ndj=ndj.p)
					if (ndj.tmp_flags&new_flags)==new_flags:break
					ndj.tmp_flags|=new_flags
					jump_mattered=1
					if ndj==nd_entry:break
					ndj_last=ndj
		if !jump_mattered:break
	//create graph for the expanded nodes
	//we only expand CFG nodes if they have TMPF_CFG_MATTERS_FOR_INTEREST: merely using an interest in the condition is no different than a basic block
	return this.dfsCreateNodeGraph(ndi,nd_entry,cached)
}

//TODO: expand call, expand loop, diversify (expand phi)
